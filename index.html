<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>üé§ Speech ‚Üí GPT-4o mini</title>

  <!-- Azure Speech SDK -->
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
  <script src="settings.js"></script>

  <style>
    /* ‚Äî‚Äî‚Äî Design tokens ‚Äî‚Äî‚Äî */
    :root{
      --bg:#fbfbfb; --fg:#111; --pane:#f1f5ff; --pane2:#fff8f3; --border:#d0d7e2;
      --accent:#2563eb; --accent-h:#1d4ed8;
    }
    @media(prefers-color-scheme:dark){
      :root{
        --bg:#0f1522; --fg:#f5f7fa; --pane:#1c2436; --pane2:#2a1e14; --border:#2b3448;
        --accent:#3b82f6; --accent-h:#60a5fa;
      }
    }

    /* ‚Äî‚Äî‚Äî Generic styles ‚Äî‚Äî‚Äî */
    *,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif;
         background:var(--bg);color:var(--fg);max-width:1000px;margin:40px auto;
         padding:0 1rem}
    h1{font-size:1.6rem;font-weight:600;display:flex;align-items:center;gap:.5rem;
        margin:0 0 1rem}
    h2{font-size:1.05rem;font-weight:600;margin:.9rem 0 .4rem}
    button{padding:.6rem 1.2rem;border:0;border-radius:.75rem;font-size:.95rem;
           background:var(--accent);color:#fff;cursor:pointer;transition:.2s}
    button:hover:not(:disabled){background:var(--accent-h)}
    button:disabled{opacity:.4;cursor:not-allowed}

    /* ‚Äî‚Äî‚Äî Layout panes ‚Äî‚Äî‚Äî */
    .pane{background:var(--pane);white-space:pre-wrap;border:1px solid var(--border);
          border-radius:1rem;padding:1rem;height:18rem;overflow:auto}
    #batchOutput{background:var(--pane2)}
    #answer{background:var(--pane2)}
    #usage{font-size:.85rem;opacity:.7;margin-top:.4rem}

    .pane::-webkit-scrollbar{width:6px}
    .pane::-webkit-scrollbar-thumb{background:var(--border);border-radius:3px}

    .grid{display:flex;gap:1rem;flex-wrap:wrap}
    .grid>div{flex:1 1 300px;min-width:250px}
  </style>
</head>
<body>
  <h1>üé§ Live Transcription ‚Üí GPT-4o mini</h1>

  <div style="display:flex;flex-wrap:wrap;gap:.5rem 1rem">
    <button id="startBtn">Start recording</button>
    <button id="stopBtn"  disabled>Stop recording</button>
    <button id="batchBtn" disabled>Improve with Batch</button>
    <button id="askBtn"   disabled>Ask GPT-4o mini</button>
  </div>

  <div class="grid">
    <div>
      <h2>Live transcript</h2>
      <div id="liveOutput" class="pane"></div>
    </div>
    <div>
      <h2>Batch transcript</h2>
      <div id="batchOutput" class="pane"></div>
    </div>
  </div>

  <h2>GPT-4o mini answer</h2>
  <div id="answer" class="pane"></div>
  <div id="usage"></div>

  <!-- -------------- JS logic -------------- -->
  <script type="module">
    import OpenAI from "https://cdn.jsdelivr.net/npm/openai@4.26.0/+esm";

    /* ---------- settings ---------- */
    const {
      speechKey, speechRegion,
      openaiKey, openaiBase, deployment, apiVersion,
      blobSasUrl
    } = window.APP_SETTINGS ?? {};
    if (!speechKey || !speechRegion || !openaiKey || !openaiBase ||
        !deployment || !apiVersion || !blobSasUrl) {
      alert("‚ö†Ô∏è Missing settings ‚Äî check settings.js");
    }

    /* ---------- dom helpers ---------- */
    const $ = id => document.getElementById(id);
    const liveEl  = $("liveOutput");
    const batchEl = $("batchOutput");
    const ansEl   = $("answer");
    const usageEl = $("usage");
    const [startBtn, stopBtn, batchBtn, askBtn] =
          ["startBtn","stopBtn","batchBtn","askBtn"].map($);
    const append = (el, txt) => { el.textContent += txt; el.scrollTop = el.scrollHeight; };

    /* ---------- live STT ---------- */
    const cfg = SpeechSDK.SpeechConfig.fromSubscription(speechKey, speechRegion);
    cfg.speechRecognitionLanguage = "en-US";
    cfg.setProperty(
      SpeechSDK.PropertyId.SpeechServiceResponse_DiarizationEnabled, "true");
    const mic  = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const stt  = new SpeechSDK.ConversationTranscriber(cfg, mic);

    const transcriptLines = [];
    stt.transcribed = (_, e) => {
      const line = `Speaker ${e.result.speakerId}: ${e.result.text}\n`;
      transcriptLines.push(line);
      append(liveEl, line);
      askBtn.disabled = false;
      batchBtn.disabled = false;
    };

    /* ---------- recording ---------- */
    let rec; let chunks = [];
    async function startRec() {
      chunks = [];
      const stream = await navigator.mediaDevices.getUserMedia({ audio:true });
      rec = new MediaRecorder(stream, { mimeType:"audio/webm" });
      rec.ondataavailable = e => chunks.push(e.data);
      rec.start();
    }
    function stopRec() {
      return new Promise(res => {
        rec.onstop = () => res(new Blob(chunks,{type:"audio/webm"}));
        rec.stop();
      });
    }

    /* ---------- blob upload ---------- */
    async function uploadBlob(blob) {
      const qi = blobSasUrl.indexOf("?");
      const root = blobSasUrl.slice(0, qi);
      const sas  = blobSasUrl.slice(qi);
      const name = `meeting-${Date.now()}.webm`;
      const url  = `${root}/${name}${sas}`;
      const r = await fetch(url, {
        method:"PUT",
        headers:{ "x-ms-blob-type":"BlockBlob" },
        body:blob
      });
      if (!r.ok) throw new Error("Blob PUT " + r.status);
      return url;
    }

    /* ---------- batch STT helpers ---------- */
    const batchEp =
      `https://${speechRegion}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions`;
    async function createBatch(fileUrl) {
      const body = {
        displayName: `Meeting ${new Date().toISOString()}`,
        contentUrls:[ fileUrl ],
        locale:"en-US",
        properties:{ diarizationEnabled:true,punctuationMode:"DictatedAndAutomatic","format": "Detailed" }
      };
      const r = await fetch(batchEp, {
        method:"POST",
        headers:{ "Ocp-Apim-Subscription-Key":speechKey,
                  "Content-Type":"application/json" },
        body:JSON.stringify(body)
      });
      if (!r.ok) throw new Error(await r.text());
      return r.headers.get("location");
    }
    async function pollJob(url) {
      while(true){
        const r = await fetch(url, { headers:{ "Ocp-Apim-Subscription-Key":speechKey }});
        const j = await r.json();
        if (j.status === "Succeeded") return j;
        if (j.status === "Failed")    throw new Error(JSON.stringify(j));
        await new Promise(s=>setTimeout(s,8000));
      }
    }
    function applyBatch(json) {
      let text = "";
      if (json.results?.transcripts?.length){
        text = json.results.transcripts.map(t=>t.transcript).join("\n");
      } else if (json.combinedRecognizedPhrases?.length){
        text = json.combinedRecognizedPhrases
          .sort((a,b)=>a.offset-b.offset)
          .map(p=>`Speaker ${p.speaker ?? p.channel ?? 0}: ${p.display}`).join("\n");
      }
      batchEl.textContent = text + "\n(Batch polished)";
      transcriptLines.length = 0;
      transcriptLines.push(...text.split(/\r?\n/).map(l=>l+"\n"));
    }

    /* ---------- GPT client ---------- */
    const openai = new OpenAI({
      apiKey: openaiKey,
      baseURL: `${openaiBase}/openai/deployments/${deployment}`,
      dangerouslyAllowBrowser:true,
      defaultQuery:{ "api-version": apiVersion }
    });
    async function askGPT() {
      ansEl.textContent = ""; usageEl.textContent = "";
      const prompt = 'TOP INSIGHTS first (max 5 bullets), then per-speaker bullets.\\nUse the speaker name before the colon if present, otherwise "Speaker <n>".';
      const r = await openai.chat.completions.create({
        model:"",
        messages:[
          {role:"system",content:"You are a concise analyst."},
          {role:"user",content:`${prompt}\n\n-----\n${transcriptLines.join("")}-----`}
        ],
        temperature:0.3
      });
      ansEl.textContent = r.choices[0].message.content.trim();
      if (r.usage){
        const {prompt_tokens:p,completion_tokens:c,total_tokens:t}=r.usage;
        usageEl.textContent = `Tokens ‚Äî prompt ${p}  completion ${c}  total ${t}`;
      }
    }

    /* ---------- buttons ---------- */
    startBtn.onclick = async () => {
      liveEl.textContent = batchEl.textContent = ansEl.textContent = "";
      usageEl.textContent = ""; transcriptLines.length = 0;
      askBtn.disabled = batchBtn.disabled = true;
      startBtn.disabled = true; stopBtn.disabled = false;
      await startRec(); stt.startTranscribingAsync();
    };
    stopBtn.onclick = async () => {
      stt.stopTranscribingAsync();
      window._audio = await stopRec();
      startBtn.disabled = false; stopBtn.disabled = true;
    };
    batchBtn.onclick = async () => {
      batchBtn.disabled = true;
      append(batchEl,"\n‚è≥ Polishing with Batch STT‚Ä¶\n");
      try{
        const fileUrl = await uploadBlob(window._audio);
        const jobUrl  = await createBatch(fileUrl);
        const jobRes  = await pollJob(jobUrl);

        /* --- fetch file list --- */
        const filesRes = await fetch(jobRes.links.files,{
          headers:{ "Ocp-Apim-Subscription-Key":speechKey }
        });
        const filesJson = await filesRes.json();
        const txFile = filesJson.values.find(f=>f.kind==="Transcription");
        if (!txFile) throw new Error("No transcription file found");
        const txRes = await fetch(txFile.links.contentUrl);
        const transcriptJson = await txRes.json();
        /* ----------------------- */

        applyBatch(transcriptJson);
        append(batchEl,"\n‚úÖ Batch transcript applied\n");
        askBtn.disabled = false;
      }catch(e){
        append(batchEl,"‚ùå Batch error: "+e);
        console.error(e);
        batchBtn.disabled = false;
      }
    };
    askBtn.onclick = askGPT;
  </script>
</body>
</html>
