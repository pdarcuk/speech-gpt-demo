<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>🎤 Speech → GPT-4o mini</title>

  <!-- Azure Speech SDK -->
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
  <script src="settings.js"></script>

  <style>
    /* ===== CSS RESET (mini) ===================================== */
    *,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
    :root{
      --bg:#fdfdfd;--fg:#111;--pane:#f1f5ff;--pane-2:#fff8f3;--border:#d0d7e2;
      --accent:#2563eb;--accent-hover:#1d4ed8;
    }
    @media(prefers-color-scheme:dark){
      :root{
        --bg:#0f1522;--fg:#f5f7fa;--pane:#1c2436;--pane-2:#2a1e14;--border:#2b3448;
        --accent:#3b82f6;--accent-hover:#60a5fa;
      }
    }
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif;
         background:var(--bg);color:var(--fg);max-width:960px;margin:40px auto;padding:0 1rem}
    h1{font-size:1.6rem;font-weight:600;display:flex;align-items:center;gap:.4rem;margin-bottom:1rem}
    h2{font-size:1.05rem;font-weight:600;margin:1.5rem 0 .5rem}
    button{padding:.6rem 1.2rem;border:none;border-radius:.75rem;font-size:.95rem;
           background:var(--accent);color:#fff;cursor:pointer;transition:.2s}
    button:hover:not(:disabled){background:var(--accent-hover)}
    button:disabled{opacity:.4;cursor:not-allowed}
    .pane{white-space:pre-wrap;border:1px solid var(--border);border-radius:1rem;padding:1rem;
          height:18rem;overflow:auto;background:var(--pane)}
    #answer{background:var(--pane-2)}
    #usage{font-size:.85rem;color:var(--fg);opacity:.7;margin-top:.4rem}
    /* scrollbar for code panes */
    .pane::-webkit-scrollbar{width:6px}
    .pane::-webkit-scrollbar-thumb{background:var(--border);border-radius:3px}
  </style>
</head>
<body>
  <h1>🎤 Live Transcription → GPT-4o mini</h1>

  <div style="display:flex;flex-wrap:wrap;gap:.5rem 1rem">
    <button id="startBtn">Start&nbsp;recording</button>
    <button id="stopBtn"  disabled>Stop&nbsp;recording</button>
    <button id="batchBtn" disabled>Improve&nbsp;with&nbsp;Batch</button>
    <button id="askBtn"   disabled>Ask&nbsp;GPT-4o&nbsp;mini</button>
  </div>

  <h2>Transcript</h2>
  <div id="output" class="pane"></div>

  <h2>GPT-4o mini answer</h2>
  <div id="answer" class="pane"></div>
  <div id="usage"></div>


  <script type="module">
    import OpenAI from "https://cdn.jsdelivr.net/npm/openai@4.26.0/+esm";

    /* -------------------------------------------------------------- */
    /* 1.  Secrets                                                    */
    /* -------------------------------------------------------------- */
    const {
      speechKey, speechRegion,
      openaiKey, openaiBase, deployment, apiVersion,
      blobSasUrl
    } = window.APP_SETTINGS ?? {};

    if (!speechKey || !speechRegion || !openaiKey || !openaiBase ||
        !deployment || !apiVersion || !blobSasUrl) {
      alert("⚠️  One or more settings are missing in settings.js — fill them in and reload.");
    }

    /* -------------------------------------------------------------- */
    /* 2.  DOM helpers                                                */
    /* -------------------------------------------------------------- */
    const $   = id => document.getElementById(id);
    const out = $("output"), ans = $("answer"), usage = $("usage");
    const [startBtn, stopBtn, batchBtn, askBtn] =
          ["startBtn","stopBtn","batchBtn","askBtn"].map($);
    const append = (el, txt) => { el.textContent += txt; el.scrollTop = el.scrollHeight; };

    /* -------------------------------------------------------------- */
    /* 3.  Speech-to-text live                                        */
    /* -------------------------------------------------------------- */
    const speechCfg = SpeechSDK.SpeechConfig.fromSubscription(speechKey, speechRegion);
    speechCfg.speechRecognitionLanguage = "en-US";
    speechCfg.setProperty(
      SpeechSDK.PropertyId.SpeechServiceResponse_DiarizationEnabled, "true"
    );
    const micAudio   = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const transcriber = new SpeechSDK.ConversationTranscriber(speechCfg, micAudio);

    const transcriptLines = [];
    transcriber.transcribed = (_, e) => {
      const name = `Speaker ${e.result.speakerId}`;
      const line = `${name}: ${e.result.text}\n`;
      transcriptLines.push(line);
      append(out, line);
      askBtn.disabled   = false;
      batchBtn.disabled = false;
    };

    /* -------------------------------------------------------------- */
    /* 4.  MediaRecorder                                              */
    /* -------------------------------------------------------------- */
    let mediaRec; let chunks = [];
    async function startRecording() {
      chunks = [];
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRec = new MediaRecorder(stream, { mimeType: "audio/webm" });
      mediaRec.ondataavailable = e => chunks.push(e.data);
      mediaRec.start();
    }
    function stopRecording() {
      return new Promise(res => {
        mediaRec.onstop = () => res(new Blob(chunks, { type: "audio/webm" }));
        mediaRec.stop();
      });
    }

    /* -------------------------------------------------------------- */
    /* 5.  Upload blob                                                */
    /* -------------------------------------------------------------- */
    async function uploadToBlob(blob) {
      const i   = blobSasUrl.indexOf("?");
      const root= blobSasUrl.substring(0, i);   // https://.../audio
      const sas = blobSasUrl.substring(i);      // ?sv=...
      const file = `meeting-${Date.now()}.webm`;
      const url  = `${root}/${file}${sas}`;

      const r = await fetch(url, {
        method: "PUT",
        headers: { "x-ms-blob-type": "BlockBlob" },
        body: blob
      });
      if (!r.ok) throw new Error("Blob PUT failed: " + r.status);
      return url;                               // read+write SAS
    }

    /* -------------------------------------------------------------- */
    /* 6.  Batch STT helpers                                          */
    /* -------------------------------------------------------------- */
    const batchEndpoint =
      `https://${speechRegion}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions`;

    async function createBatchJob(fileUrl) {
      const body = {
        displayName: `Meeting ${new Date().toISOString()}`,
        contentUrls: [fileUrl],
        locale:      "en-US",
        properties:  { diarizationEnabled: true, punctuationMode: "DictatedAndAutomatic" }
      };
      const r = await fetch(batchEndpoint, {
        method: "POST",
        headers: { "Ocp-Apim-Subscription-Key": speechKey,
                   "Content-Type": "application/json" },
        body: JSON.stringify(body)
      });
      if (!r.ok) throw new Error(await r.text());
      return r.headers.get("location");
    }

    async function pollJob(jobUrl) {
      while (true) {
        const r = await fetch(jobUrl, {
          headers: { "Ocp-Apim-Subscription-Key": speechKey }
        });
        const j = await r.json();
        if (j.status === "Succeeded") return j;
        if (j.status === "Failed")    throw new Error(JSON.stringify(j, null, 2));
        await new Promise(s => setTimeout(s, 8000));
      }
    }

    function applyBatchTranscript(json) {
      let text = "";
      if (json.results?.transcripts?.length) {
        text = json.results.transcripts.map(t => t.transcript).join("\n");
      } else if (json.combinedRecognizedPhrases?.length) {
        text = json.combinedRecognizedPhrases.map(p => p.display).join("\n");
      } else {
        text = "(No transcript text in Batch JSON)";
      }
      out.textContent = text + "\n(Batch polished)";
      transcriptLines.length = 0;
      transcriptLines.push(...text.split(/\r?\n/).map(l => l + "\n"));
    }

    /* -------------------------------------------------------------- */
    /* 7.  GPT client                                                 */
    /* -------------------------------------------------------------- */
    const openai = new OpenAI({
      apiKey: openaiKey,
      baseURL: `${openaiBase}/openai/deployments/${deployment}`,
      dangerouslyAllowBrowser: true,
      defaultQuery: { "api-version": apiVersion }
    });

    async function askGPT() {
      ans.textContent = ""; usage.textContent = "";
      const prompt = 'TOP INSIGHTS first (max 5 bullets), then per-speaker bullets.\\nUse the speaker name before the colon if present, otherwise "Speaker <n>".';

      const resp = await openai.chat.completions.create({
        model: "",
        messages: [
          { role: "system", content: "You are a concise analyst." },
          { role: "user", content: `${prompt}\n\n-----\n${transcriptLines.join("")}-----` }
        ],
        temperature: 0.3
      });

      ans.textContent = resp.choices[0].message.content.trim();
      if (resp.usage) {
        const { prompt_tokens:p, completion_tokens:c, total_tokens:t } = resp.usage;
        usage.textContent = `Tokens — prompt ${p}  completion ${c}  total ${t}`;
      }
    }

    /* -------------------------------------------------------------- */
    /* 8.  Buttons                                                    */
    /* -------------------------------------------------------------- */
    startBtn.onclick = async () => {
      out.textContent = ""; ans.textContent = ""; usage.textContent = "";
      transcriptLines.length = 0; askBtn.disabled = true; batchBtn.disabled = true;
      startBtn.disabled = true;  stopBtn.disabled = false;
      await startRecording();
      transcriber.startTranscribingAsync();
    };

    stopBtn.onclick = async () => {
      transcriber.stopTranscribingAsync();
      window._lastAudioBlob = await stopRecording();
      startBtn.disabled = false; stopBtn.disabled = true;
    };

    batchBtn.onclick = async () => {
      batchBtn.disabled = true;
      append(out, "\n⏳ Polishing with Batch STT…\n");
      try {
        const fileUrl   = await uploadToBlob(window._lastAudioBlob);
        const jobUrl    = await createBatchJob(fileUrl);
        const jobResult = await pollJob(jobUrl);

        /* ---------- new file-list logic ---------- */
        const filesResp = await fetch(jobResult.links.files, {
          headers: { "Ocp-Apim-Subscription-Key": speechKey }
        });
        const filesJson = await filesResp.json();
        const txFile    = filesJson.values.find(f => f.kind === "Transcription");
        if (!txFile) throw new Error("No transcription file found in job result");

        const transcriptResp = await fetch(txFile.links.contentUrl);
        const transcriptJson = await transcriptResp.json();
        /* ---------------------------------------- */

        applyBatchTranscript(transcriptJson);
        append(out, "\n✅ Batch transcript applied\n");
        askBtn.disabled = false;
      } catch (err) {
        append(out, "❌ Batch error: " + err);
        console.error(err);
        batchBtn.disabled = false;
      }
    };

    askBtn.onclick = askGPT;
  </script>
</body>
</html>
