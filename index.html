<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ğŸ¤ Speech â†’ GPTâ€‘4o mini</title>
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
  <script src="settings.js"></script>
  <style>
    :root{--bg:#fbfbfb;--fg:#111;--pane:#f1f5ff;--pane2:#fff8f3;--border:#d0d7e2;--accent:#2563eb;--accent-h:#1d4ed8}
    @media(prefers-color-scheme:dark){:root{--bg:#0f1522;--fg:#f5f7fa;--pane:#1c2436;--pane2:#2a1e14;--border:#2b3448;--accent:#3b82f6;--accent-h:#60a5fa}}
    *,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif;background:var(--bg);color:var(--fg);max-width:1000px;margin:40px auto;padding:0 1rem}
    h1{font-size:1.6rem;font-weight:600;margin:0 0 1rem}
    h2{font-size:1.05rem;font-weight:600;margin:.9rem 0 .4rem}
    button{padding:.6rem 1.1rem;border:0;border-radius:.75rem;font-size:.9rem;background:var(--accent);color:#fff;cursor:pointer;transition:.2s}
    button:hover:not(:disabled){background:var(--accent-h)}button:disabled{opacity:.4;cursor:not-allowed}
    .pane{background:var(--pane);white-space:pre-wrap;border:1px solid var(--border);border-radius:1rem;padding:1rem;height:18rem;overflow:auto}
    #batchOutput{background:var(--pane2)}#answer{background:var(--pane2)}#usage{font-size:.85rem;opacity:.7;margin-top:.4rem}
    .pane::-webkit-scrollbar{width:6px}.pane::-webkit-scrollbar-thumb{background:var(--border);border-radius:3px}
    .grid{display:flex;gap:1rem;flex-wrap:wrap}.grid>div{flex:1 1 300px;min-width:250px}
  </style>
</head>
<body>
  <h1>ğŸ¤ Live Transcription â†’ GPTâ€‘4o mini</h1>

  <h2>Live transcript</h2>
  <button id="startBtn">Start recording</button>
  <button id="stopBtn" disabled>Stop recording</button>
  <div id="liveOutput" class="pane" style="margin-top:.6rem"></div>

  <h2 style="margin-top:1.4rem">Batch transcript</h2>
  <button id="batchBtn" disabled>Improve with Batch</button>
  <div id="batchOutput" class="pane" style="margin-top:.6rem"></div>

  <h2 style="margin-top:1.4rem">GPTâ€‘4o mini answer</h2>
  <label style="display:inline-flex;align-items:center;gap:.4rem;margin-bottom:.4rem">
    <input type="checkbox" id="appendCheck"/>
    Append AIâ€‘cleaned transcript
  </label>
  <button id="askBtn" disabled>Ask GPTâ€‘4o mini</button>
  <div id="answer" class="pane" style="margin-top:.6rem"></div>
  <div id="usage"></div>

  <!-- -------------- JS logic -------------- -->
  <script type="module">
    import OpenAI from "https://cdn.jsdelivr.net/npm/openai@4.26.0/+esm";

    /* ---------- settings ---------- */
    const {
      speechKey,speechRegion,openaiKey,openaiBase,
      deployment,apiVersion,blobSasUrl
    }=window.APP_SETTINGS??{};
    if(!speechKey||!speechRegion||!openaiKey||!openaiBase||
       !deployment||!apiVersion||!blobSasUrl){
      alert("Missing settings â€” check settings.js");
    }

    /* ---------- dom ---------- */
    const $ = id => document.getElementById(id);
    const liveEl   = $("liveOutput");
    const batchEl  = $("batchOutput");
    const ansEl    = $("answer");
    const usageEl  = $("usage");
    const appendCb = $("appendCheck");
    const [startBtn,stopBtn,batchBtn,askBtn] =
          ["startBtn","stopBtn","batchBtn","askBtn"].map($);
    const append = (el,txt)=>{el.textContent+=txt;el.scrollTop=el.scrollHeight};

    /* ---------- live STT ---------- */
    const cfg = SpeechSDK.SpeechConfig.fromSubscription(speechKey,speechRegion);
    cfg.speechRecognitionLanguage="en-US";
    cfg.setProperty(
      SpeechSDK.PropertyId.SpeechServiceResponse_DiarizationEnabled,"true");
    const mic  = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const stt  = new SpeechSDK.ConversationTranscriber(cfg,mic);

    const liveLines=[], batchLines=[];
    stt.transcribed = (_,e)=>{
      const line=`Speaker ${e.result.speakerId}: ${e.result.text}\n`;
      liveLines.push(line);
      append(liveEl,line);
      batchBtn.disabled = askBtn.disabled = false;
    };

    /* ---------- recording ---------- */
    let rec; let chunks=[];
    async function startRec(){
      chunks=[];
      const stream=await navigator.mediaDevices.getUserMedia({audio:true});
      rec=new MediaRecorder(stream,{mimeType:"audio/webm"});
      rec.ondataavailable=e=>chunks.push(e.data);
      rec.start();
    }
    function stopRec(){
      return new Promise(res=>{
        rec.onstop=()=>res(new Blob(chunks,{type:"audio/webm"}));
        rec.stop();
      });
    }

    /* ---------- blob upload ---------- */
    async function uploadBlob(blob){
      const qi=blobSasUrl.indexOf("?");
      const root=blobSasUrl.slice(0,qi);
      const sas =blobSasUrl.slice(qi);
      const name=`meet-${Date.now()}.webm`;
      const url =`${root}/${name}${sas}`;
      const r=await fetch(url,{
        method:"PUT",
        headers:{ "x-ms-blob-type":"BlockBlob" },
        body:blob
      });
      if(!r.ok)throw new Error("Blob PUT "+r.status);
      return url;
    }

    /* ---------- batch ---------- */
    const ep=`https://${speechRegion}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions`;
    async function createJob(u){
      const body={
        displayName:`Meeting ${new Date().toISOString()}`,
        contentUrls:[u],
        locale:"en-US",
        properties:{
          diarizationEnabled:true,
          punctuationMode:"DictatedAndAutomatic",
          format:"Detailed"
        }
      };
      const r=await fetch(ep,{
        method:"POST",
        headers:{
          "Ocp-Apim-Subscription-Key":speechKey,
          "Content-Type":"application/json"
        },
        body:JSON.stringify(body)
      });
      if(!r.ok)throw new Error(await r.text());
      return r.headers.get("location");
    }
    async function poll(j){
      while(true){
        const r=await fetch(j,{headers:{"Ocp-Apim-Subscription-Key":speechKey}});
        const d=await r.json();
        if(d.status==="Succeeded")return d;
        if(d.status==="Failed")throw new Error(JSON.stringify(d));
        await new Promise(s=>setTimeout(s,8000));
      }
    }
    function showBatch(js){
      let text="";
      if(js.combinedRecognizedPhrases?.length){
        text=js.combinedRecognizedPhrases
              .sort((a,b)=>a.offset-b.offset)
              .map(p=>`Speaker ${p.speaker??p.channel??0}: ${p.display}`)
              .join("\n");
      }else if(js.results?.transcripts?.length){
        text=js.results.transcripts.map(t=>t.transcript).join("\n");
      }
      batchLines.length=0;
      batchLines.push(...text.split(/\r?\n/).map(l=>l+"\n"));
      batchEl.textContent=text+"\n(Batch polished)";
    }

    /* ---------- GPT ---------- */
    const oa=new OpenAI({
      apiKey:openaiKey,
      baseURL:`${openaiBase}/openai/deployments/${deployment}`,
      dangerouslyAllowBrowser:true,
      defaultQuery:{ "api-version":apiVersion }
    });
    async function ask(){
      ansEl.textContent=""; usageEl.textContent="";
      const wantTranscript=appendCb.checked;

      /* ---- Build prompt ---- */
      const promptParts=[
        "Provide:",
        "1. Top 5 insights from the meeting as concise bullets.",
        "2. Insights per speaker (label each speaker)."
      ];
      if(wantTranscript){
        promptParts.push(
          "3. Then append under the heading '--- AI-Cleaned Transcript ---' a cleaned, speaker-labelled full transcript. " +
          "Use the LIVE transcript for speaker names, but fix obvious errors using the BATCH transcript."
        );
      }
      const fullUserPrompt = `${promptParts.join("\\n")}
\\n----- LIVE TRANSCRIPT -----
${liveLines.join("")}
----- BATCH TRANSCRIPT -----
${batchLines.join("")}
-----`;

      const r=await oa.chat.completions.create({
        model:"",
        temperature:0.3,
        messages:[
          {role:"system",content:"You are a concise analyst."},
          {role:"user",content:fullUserPrompt}
        ]
      });

      ansEl.textContent=r.choices[0].message.content.trim();
      if(r.usage){
        const {prompt_tokens:p,completion_tokens:c,total_tokens:t}=r.usage;
        usageEl.textContent=`Tokens â€” prompt ${p}  completion ${c}  total ${t}`;
      }
    }

    /* ---------- buttons ---------- */
    startBtn.onclick=async()=>{
      liveEl.textContent=batchEl.textContent=ansEl.textContent="";
      usageEl.textContent="";
      liveLines.length=batchLines.length=0;
      batchBtn.disabled=askBtn.disabled=true;
      startBtn.disabled=true; stopBtn.disabled=false;
      await startRec(); stt.startTranscribingAsync();
    };
    stopBtn.onclick=async()=>{
      stt.stopTranscribingAsync();
      window._audio=await stopRec();
      startBtn.disabled=false; stopBtn.disabled=true;
    };
    batchBtn.onclick=async()=>{
      batchBtn.disabled=true;
      append(batchEl,"\\nâ³ Polishing with Batch STTâ€¦\\n");
      try{
        const url  = await uploadBlob(window._audio);
        const job  = await createJob(url);
        const res  = await poll(job);

        const filesRes=await fetch(res.links.files,{
          headers:{ "Ocp-Apim-Subscription-Key":speechKey }
        });
        const filesJson=await filesRes.json();
        const txFile=filesJson.values.find(f=>f.kind==="Transcription");
        if(!txFile)throw new Error("No transcription file found");
        const txRes=await fetch(txFile.links.contentUrl);
        const txJson=await txRes.json();
        showBatch(txJson);
        append(batchEl,"\\nâœ… Batch transcript applied\\n");
        askBtn.disabled=false;
      }catch(e){
        append(batchEl,"âŒ Batch error: "+e);
        console.error(e);
        batchBtn.disabled=false;
      }
    };
    askBtn.onclick=ask;
  </script>
</body>
</html>

</body>
</html>

