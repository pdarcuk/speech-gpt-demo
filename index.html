<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>🎤 Speech → GPT‑4o mini</title>

  <!-- Azure Speech SDK -->
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
  <script src="settings.js"></script>

  <style>
    /* ===== CSS RESET (mini) ===================================== */
    *,*::before,*::after{box-sizing:border-box;margin:0;padding:0}
    :root{
      --bg:#fdfdfd;--fg:#111;--pane:#f1f5ff;--pane-2:#fff8f3;--border:#d0d7e2;
      --accent:#2563eb;--accent-hover:#1d4ed8;
    }
    @media(prefers-color-scheme:dark){
      :root{
        --bg:#0f1522;--fg:#f5f7fa;--pane:#1c2436;--pane-2:#2a1e14;--border:#2b3448;
        --accent:#3b82f6;--accent-hover:#60a5fa;
      }
    }
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif;
         background:var(--bg);color:var(--fg);max-width:1000px;margin:40px auto;padding:0 1rem}
    h1{font-size:1.6rem;font-weight:600;display:flex;align-items:center;gap:.4rem;margin-bottom:1rem}
    h2{font-size:1.05rem;font-weight:600;margin:.9rem 0 .4rem}
    button{padding:.6rem 1.2rem;border:none;border-radius:.75rem;font-size:.95rem;
           background:var(--accent);color:#fff;cursor:pointer;transition:.2s}
    button:hover:not(:disabled){background:var(--accent-hover)}
    button:disabled{opacity:.4;cursor:not-allowed}
    .pane{white-space:pre-wrap;border:1px solid var(--border);border-radius:1rem;padding:1rem;
          height:18rem;overflow:auto;background:var(--pane)}
    #answer{background:var(--pane-2)}
    #usage{font-size:.85rem;color:var(--fg);opacity:.7;margin-top:.4rem}
    .pane::-webkit-scrollbar{width:6px}
    .pane::-webkit-scrollbar-thumb{background:var(--border);border-radius:3px}
    .grid{display:flex;gap:1rem;flex-wrap:wrap}
    .grid>div{flex:1 1 300px;min-width:250px}
  </style>
</head>
<body>
  <h1>🎤 Live Transcription → GPT‑4o mini</h1>

  <div style="display:flex;flex-wrap:wrap;gap:.5rem 1rem">
    <button id="startBtn">Start&nbsp;recording</button>
    <button id="stopBtn"  disabled>Stop&nbsp;recording</button>
    <button id="batchBtn" disabled>Improve&nbsp;with&nbsp;Batch</button>
    <button id="askBtn"   disabled>Ask&nbsp;GPT‑4o&nbsp;mini</button>
  </div>

  <div class="grid">
    <div>
      <h2>Live transcript</h2>
      <div id="liveOutput" class="pane"></div>
    </div>
    <div>
      <h2>Batch transcript</h2>
      <div id="batchOutput" class="pane"></div>
    </div>
  </div>

  <h2>GPT‑4o mini answer</h2>
  <div id="answer" class="pane"></div>
  <div id="usage"></div>

  <!-- ——  JS logic  —— -->
  <script type="module">
    import OpenAI from "https://cdn.jsdelivr.net/npm/openai@4.26.0/+esm";

    const { speechKey,speechRegion,openaiKey,openaiBase,deployment,apiVersion,blobSasUrl } = window.APP_SETTINGS ?? {};
    if(!speechKey||!speechRegion||!openaiKey||!openaiBase||!deployment||!apiVersion||!blobSasUrl){alert("⚠️ Missing settings");}

    const $=id=>document.getElementById(id);
    const liveEl=$("liveOutput"),batchEl=$("batchOutput"),ansEl=$("answer"),usageEl=$("usage");
    const [startBtn,stopBtn,batchBtn,askBtn] = ["startBtn","stopBtn","batchBtn","askBtn"].map($);
    const append=(el,txt)=>{el.textContent+=txt;el.scrollTop=el.scrollHeight;};

    /* ---- Speech SDK ---- */
    const speechCfg=SpeechSDK.SpeechConfig.fromSubscription(speechKey,speechRegion);
    speechCfg.speechRecognitionLanguage="en-US";
    speechCfg.setProperty(SpeechSDK.PropertyId.SpeechServiceResponse_DiarizationEnabled,"true");
    const mic=SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
    const transcriber=new SpeechSDK.ConversationTranscriber(speechCfg,mic);

    const transcriptLines=[];
    transcriber.transcribed = (_,e)=>{
      const line=`Speaker ${e.result.speakerId}: ${e.result.text}\n`;
      transcriptLines.push(line);
      append(liveEl,line);
      askBtn.disabled=false;batchBtn.disabled=false;
    };

    /* ---- Recording ---- */
    let mediaRec;let chunks=[];
    async function startRec(){chunks=[];const s=await navigator.mediaDevices.getUserMedia({audio:true});mediaRec=new MediaRecorder(s,{mimeType:"audio/webm"});mediaRec.ondataavailable=e=>chunks.push(e.data);mediaRec.start();}
    function stopRec(){return new Promise(r=>{mediaRec.onstop=()=>r(new Blob(chunks,{type:"audio/webm"}));mediaRec.stop();});}

    /* ---- Blob upload ---- */
    async function uploadBlob(blob){const i=blobSasUrl.indexOf("?");const root=blobSasUrl.slice(0,i);const sas=blobSasUrl.slice(i);const name=`meeting-${Date.now()}.webm`;const url=`${root}/${name}${sas}`;const res=await fetch(url,{method:"PUT",headers:{"x-ms-blob-type":"BlockBlob"},body:blob});if(!res.ok)throw new Error("Blob PUT "+res.status);return url;}

    /* ---- Batch helpers ---- */
    const batchEndpoint=`https://${speechRegion}.api.cognitive.microsoft.com/speechtotext/v3.1/transcriptions`;
    async function createBatch(fileUrl){const body={displayName:`Meeting ${new Date().toISOString()}`,contentUrls:[fileUrl],locale:"en-US",properties:{diarizationEnabled:true,punctuationMode:"DictatedAndAutomatic"}};const r=await fetch(batchEndpoint,{method:"POST",headers:{"Ocp-Apim-Subscription-Key":speechKey,"Content-Type":"application/json"},body:JSON.stringify(body)});if(!r.ok)throw new Error(await r.text());return r.headers.get("location");}
    async function poll(url){while(true){const r=await fetch(url,{headers:{"Ocp-Apim-Subscription-Key":speechKey}});const j=await r.json();if(j.status==="Succeeded")return j;if(j.status==="Failed")throw new Error(JSON.stringify(j));await new Promise(s=>setTimeout(s,8000));}}
    function applyBatch(json){let text="";if(json.results?.transcripts?.length){text=json.results.transcripts.map(t=>t.transcript).join("\n");}else if(json.combinedRecognizedPhrases?.length){text=json.combinedRecognizedPhrases.sort((a,b)=>a.offset-b.offset).map(p=>`Speaker ${p.speaker??p.channel??0}: ${p.display}`).join("\n");}batchEl.textContent=text+"\n(Batch polished)";transcriptLines.length=0;transcriptLines.push(...text.split(/\r?\n/).map(l=>l+"\n"));}

    /* ---- GPT client ---- */
    const openai=new OpenAI({apiKey:openaiKey,baseURL:`${openaiBase}/openai/deployments/${deployment}`,dangerouslyAllowBrowser:true,defaultQuery:{"api-version":apiVersion}});
    async function askGPT(){ansEl.textContent="";usageEl.textContent="";const prompt='TOP INSIGHTS first (max 5 bullets), then per-speaker bullets.\nUse the speaker name before the colon if present, otherwise "Speaker <n>".';const r=await openai.chat.completions.create({model:"",messages:[{role:"system",content:"You are a concise analyst."},{role:"user",content:`${prompt}\n\n-----\n${transcriptLines.join("")}-----`}],temperature:0.3});ansEl.textContent=r.choices[0].message.content.trim();if(r.usage){const {prompt_tokens:p,completion_tokens:c,total_tokens:t}=r.usage;usageEl.textContent=`Tokens — prompt ${p}  completion ${c}  total ${t}`;}}

    /* ---- Buttons ---- */
    startBtn.onclick=async()=>{liveEl.textContent=batchEl.textContent="";ansEl.textContent="";usageEl.textContent="";transcriptLines.length=0;askBtn.disabled=batchBtn.disabled=true;startBtn.disabled=true;stopBtn.disabled=false;await startRec();transcriber.startTranscribingAsync();};
    stopBtn.onclick=async()=>{transcriber.stopTranscribingAsync();window._audio=await stopRec();startBtn.disabled=false;stopBtn.disabled=true;};
    batchBtn.onclick=async()=>{batchBtn.disabled=true;append(batchEl,"\n⏳ Polishing with Batch STT…\n");try{const url=await uploadBlob(window._audio);const job=await createBatch(url);const res=await poll(job);const filesResp = await fetch(res.links.files, {
        headers: { "Ocp-Apim-Subscription-Key": speechKey }
      });
      const files = await filesResp.json();applyBatch(txJson);append(batchEl,"\n✅ Batch transcript applied\n");askBtn.disabled=false;}catch(e){append(batchEl,"❌ Batch error: "+e);console.error(e);batchBtn.disabled=false;}};
    askBtn.onclick=askGPT;
  </script>
</body>
</html>

